{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f38c7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 01:52:09.577644: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-26 01:52:09.589011: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-26 01:52:09.598790: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-26 01:52:09.634056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748224329.684604    7456 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748224329.699396    7456 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748224329.729367    7456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748224329.729396    7456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748224329.729398    7456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748224329.729400    7456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-26 01:52:09.736001: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e7c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46432de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1FSwCLJzPzL4ZmihRgjIkh3DImRwM0UHb\n",
      "From (redirected): https://drive.google.com/uc?id=1FSwCLJzPzL4ZmihRgjIkh3DImRwM0UHb&confirm=t&uuid=b82ab01f-e6eb-45ec-bd33-9f689a955de1\n",
      "To: /tmp/svhn_data/train_32x32.mat\n",
      "100%|██████████| 182M/182M [00:02<00:00, 63.2MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=10E7i5m1HaGWZkte4P8WMouNoFj0dFHmv\n",
      "From (redirected): https://drive.google.com/uc?id=10E7i5m1HaGWZkte4P8WMouNoFj0dFHmv&confirm=t&uuid=7243f1bc-16d1-4d81-bd0f-1d4a05dfb48a\n",
      "To: /tmp/svhn_data/test_32x32.mat\n",
      "100%|██████████| 64.3M/64.3M [00:01<00:00, 35.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os, gdown\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# e.g. store in /tmp or ../data\n",
    "cache_dir = \"/tmp/svhn_data\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "gdown.download(\"https://drive.google.com/uc?id=1FSwCLJzPzL4ZmihRgjIkh3DImRwM0UHb\",\n",
    "               os.path.join(cache_dir, \"train_32x32.mat\"), quiet=False)\n",
    "gdown.download(\"https://drive.google.com/uc?id=10E7i5m1HaGWZkte4P8WMouNoFj0dFHmv\",\n",
    "               os.path.join(cache_dir, \"test_32x32.mat\"), quiet=False)\n",
    "\n",
    "train_raw = loadmat(os.path.join(cache_dir, \"train_32x32.mat\"))\n",
    "test_raw  = loadmat(os.path.join(cache_dir, \"test_32x32.mat\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ca917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels\n",
    "\n",
    "train_images = np.array(train_raw['X'])\n",
    "test_images = np.array(test_raw['X'])\n",
    "\n",
    "train_labels = train_raw['y']\n",
    "test_labels = test_raw['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b6c1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3, 73257)\n",
      "(32, 32, 3, 26032)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the data\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6132a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73257, 32, 32, 3)\n",
      "(26032, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Fix the axes of the images\n",
    "\n",
    "train_images = np.moveaxis(train_images, -1, 0)\n",
    "test_images = np.moveaxis(test_images, -1, 0)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b983aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [5]\n"
     ]
    }
   ],
   "source": [
    "# Plot a random image and its label\n",
    "\n",
    "plt.imshow(train_images[13529])\n",
    "plt.show()\n",
    "\n",
    "print('Label: ', train_labels[13529])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771a3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and test images into 'float64' type\n",
    "\n",
    "train_images = train_images.astype('float64')\n",
    "test_images = test_images.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd47b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and test labels into 'int64' type\n",
    "\n",
    "train_labels = train_labels.astype('int64')\n",
    "test_labels = test_labels.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c71be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0, Max: 255.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize the images data\n",
    "\n",
    "print('Min: {}, Max: {}'.format(train_images.min(), train_images.max()))\n",
    "\n",
    "train_images /= 255.0\n",
    "test_images /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d3eed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of train and test labels\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "train_labels = lb.fit_transform(train_labels)\n",
    "test_labels = lb.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1da0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data into train and validation sets\n",
    "\n",
    "# Just try splitting the first 1000 samples for debugging\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images[:1000], train_labels[:1000],\n",
    "                                                  test_size=0.15, random_state=22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e326ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "932125f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=8,\n",
    "                             zoom_range=[0.95, 1.05],\n",
    "                             height_shift_range=0.10,\n",
    "                             shear_range=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7ac752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-05-26 01:52:37.199545: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.0001}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m      5\u001b[39m aux_model = keras.Sequential([\n\u001b[32m      6\u001b[39m     keras.layers.Conv2D(\u001b[32m32\u001b[39m, (\u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m), padding=\u001b[33m'\u001b[39m\u001b[33msame\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m      7\u001b[39m                            activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     keras.layers.Dense(\u001b[32m10\u001b[39m,  activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     35\u001b[39m ])\n\u001b[32m     37\u001b[39m lr_schedule = keras.callbacks.LearningRateScheduler(\n\u001b[32m     38\u001b[39m               \u001b[38;5;28;01mlambda\u001b[39;00m epoch: \u001b[32m1e-4\u001b[39m * \u001b[32m10\u001b[39m**(epoch / \u001b[32m10\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m optimizer = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m aux_model.compile(optimizer=optimizer,\n\u001b[32m     41\u001b[39m                   loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     42\u001b[39m                  metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/optimizers/adam.py:62\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     45\u001b[39m     learning_rate=\u001b[32m0.001\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     **kwargs,\n\u001b[32m     61\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m.beta_1 = beta_1\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m.beta_2 = beta_2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:21\u001b[39m, in \u001b[36mTFOptimizer.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mself\u001b[39m._distribution_strategy = tf.distribute.get_strategy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:90\u001b[39m, in \u001b[36mBaseOptimizer.__init__\u001b[39m\u001b[34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m     warnings.warn(\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m     )\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     name = auto_name(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Argument(s) not recognized: {'lr': 0.0001}"
     ]
    }
   ],
   "source": [
    "# Define auxillary model\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "aux_model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', \n",
    "                           activation='relu',\n",
    "                           input_shape=(32, 32, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', \n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same', \n",
    "                           activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same',\n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same', \n",
    "                           activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same',\n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),    \n",
    "    keras.layers.Dense(10,  activation='softmax')\n",
    "])\n",
    "\n",
    "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
    "              lambda epoch: 1e-4 * 10**(epoch / 10))\n",
    "optimizer = keras.optimizers.Adam(lr=1e-4, amsgrad=True)\n",
    "aux_model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5609a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model in order to determine best learning rate\n",
    "\n",
    "history = aux_model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n",
    "                              epochs=30, validation_data=(X_val, y_val),\n",
    "                              callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292838ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Learning Rate vs. Loss\n",
    "\n",
    "plt.semilogx(history.history['lr'], history.history['loss'])\n",
    "plt.axis([1e-4, 1e-1, 0, 4])\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define actual model\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', \n",
    "                           activation='relu',\n",
    "                           input_shape=(32, 32, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', \n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same', \n",
    "                           activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same',\n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same', \n",
    "                           activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same',\n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),    \n",
    "    keras.layers.Dense(10,  activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=8)\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3, amsgrad=True)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "                   '/kaggle/working/best_cnn.h5', \n",
    "                   save_best_only=True)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36744477",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ba9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model in order to make predictions\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n",
    "                              epochs=70, validation_data=(X_val, y_val),\n",
    "                              callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate train and validation accuracies and losses\n",
    "\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfcfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize epochs vs. train and validation accuracies and losses\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Epochs vs. Training and Validation Accuracy')\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Epochs vs. Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39459883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "test_loss, test_acc = model.evaluate(x=test_images, y=test_labels, verbose=0)\n",
    "\n",
    "print('Test accuracy is: {:0.4f} \\nTest loss is: {:0.4f}'.\n",
    "      format(test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f2850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and apply inverse transformation to the labels\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "y_pred = lb.inverse_transform(y_pred, lb.classes_)\n",
    "y_train = lb.inverse_transform(y_train, lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d05321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(y_train, y_pred, labels=lb.classes_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(matrix, annot=True, cmap='Greens', fmt='d', ax=ax)\n",
    "plt.title('Confusion Matrix for training dataset')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the errors in the plots\n",
    "\n",
    "np.seterr(all='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get convolutional layers\n",
    "\n",
    "layers = [model.get_layer('conv2d_1'), \n",
    "          model.get_layer('conv2d_2'),\n",
    "          model.get_layer('conv2d_3'),\n",
    "          model.get_layer('conv2d_4'),\n",
    "          model.get_layer('conv2d_5'),\n",
    "          model.get_layer('conv2d_6')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model which gives the outputs of the layers\n",
    "\n",
    "layer_outputs = [layer.output for layer in layers]\n",
    "activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the names of the layers\n",
    "\n",
    "layer_names = []\n",
    "for layer in layers:\n",
    "    layer_names.append(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59058904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which will plot the convolutional filters\n",
    "\n",
    "def plot_convolutional_filters(img):\n",
    "    \n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    activations = activation_model.predict(img)\n",
    "    images_per_row = 9\n",
    "    \n",
    "    for layer_name, layer_activation in zip(layer_names, activations): \n",
    "        n_features = layer_activation.shape[-1]\n",
    "        size = layer_activation.shape[1]\n",
    "        n_cols = n_features // images_per_row\n",
    "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "        for col in range(n_cols): \n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0,\n",
    "                                                 :, :,\n",
    "                                                 col * images_per_row + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size : (col + 1) * size,\n",
    "                             row * size : (row + 1) * size] = channel_image\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                            scale * display_grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X_train[42500]\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287fa77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convolutional_filters(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a059ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
